{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import keras\n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras import losses, models, optimizers\n",
    "from keras.layers import Dense, Input, Convolution2D, BatchNormalization, Flatten, MaxPool2D, Activation, Reshape, Dropout\n",
    "from keras.utils import plot_model, Sequence\n",
    "from keras.layers.merge import concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for dumping and loading pickle files of exceedingly large sizes\n",
    "\n",
    "Reads data byte by byte and writes data byte by byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacOSFile(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.f, item)\n",
    "\n",
    "    def read(self, n):\n",
    "        if n >= (1 << 31):\n",
    "            buffer = bytearray(n)\n",
    "            idx = 0\n",
    "            while idx < n:\n",
    "                batch_size = min(n - idx, 1 << 31 - 1)\n",
    "                buffer[idx:idx + batch_size] = self.f.read(batch_size)\n",
    "                idx += batch_size\n",
    "            return buffer\n",
    "        return self.f.read(n)\n",
    "\n",
    "    def write(self, buffer):\n",
    "        n = len(buffer)\n",
    "        print(\"writing total_bytes=%s...\" % n, flush=True)\n",
    "        idx = 0\n",
    "        while idx < n:\n",
    "            batch_size = min(n - idx, 1 << 31 - 1)\n",
    "            print(\"writing bytes [%s, %s)... \" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "            self.f.write(buffer[idx:idx + batch_size])\n",
    "            print(\"done.\", flush=True)\n",
    "            idx += batch_size\n",
    "\n",
    "\n",
    "def pickle_dump(obj, file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        return pickle.dump(obj, MacOSFile(f), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def pickle_load(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(MacOSFile(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle_load('./data/test_image.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2000\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))\n",
    "print(len(dataset))\n",
    "print(len(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the dataset and the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(dataset)):\n",
    "    X.append(np.reshape(np.array(dataset[i]), (28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the dataset is in correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping finally and also scaling it as was done in the training part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(X , (2000,28,28,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefining model architecture to load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_definition(learning_rate):\n",
    "\n",
    "    input = Input(shape=(28, 28, 1))\n",
    "\n",
    "    x = (Convolution2D(32, (3, 3), padding=\"same\", input_shape=(28, 28, 1)))(input)\n",
    "    x = (BatchNormalization())(x)\n",
    "    x = (Activation('relu'))(x)\n",
    "    x = (MaxPool2D())(x)\n",
    "    \n",
    "    x = (Dropout(0.25))(x)\n",
    "    \n",
    "    x = (Convolution2D(64, (3, 3), padding=\"same\"))(x)\n",
    "    x = (BatchNormalization())(x)\n",
    "    x = (Activation('relu'))(x)\n",
    "    \n",
    "    x = (MaxPool2D())(x)\n",
    "    x = (Dropout(0.25))(x)\n",
    "    \n",
    "    x = (Convolution2D(128, (3, 3), padding=\"same\"))(x)\n",
    "    x = (BatchNormalization())(x)\n",
    "    x = (Activation('relu'))(x)\n",
    "    \n",
    "    x = (Dropout(0.4))(x)\n",
    "\n",
    "    x = (Flatten())(x)\n",
    "    \n",
    "    x = (Dense(256, activation='relu'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = (Dropout(0.5))(x)\n",
    "\n",
    "    x = (Dense(64, activation='relu'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = (Dropout(0.4))(x)\n",
    "\n",
    "    x = (Dense(16, activation='relu'))(x)\n",
    "    x = (Dropout(0.3))(x)\n",
    "\n",
    "    output = (Dense(4, activation='softmax'))(x)\n",
    "\n",
    "    model = models.Model(inputs=input, outputs=output)\n",
    "\n",
    "    opt = keras.optimizers.RMSprop(learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 1,718,292\n",
      "Trainable params: 1,717,204\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_definition(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./data/model-90-89.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions from the model in the shape of (batch_size x 4) as they will be one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dictionary which will give us the correct class from the encoded class.\n",
    "\n",
    "We traverse the predicted numpy array and chose the index which has the maximum softmax value as the output.\n",
    "\n",
    "We append the results in y_out and the increasing indices in image_idx lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = (pickle_load('./data/dataset_large.pkl'))['dict']\n",
    "image_idx = []\n",
    "y_out = []\n",
    "ctr = 0\n",
    "for y in y_pred:\n",
    "    image_idx.append(ctr)\n",
    "    ctr += 1\n",
    "    mmax = -1\n",
    "    mmax_idx = -1\n",
    "    for i in range(4):\n",
    "        if mmax < y[i]:\n",
    "            mmax = y[i]\n",
    "            mmax_idx = i\n",
    "    y_out.append(dict[mmax_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the results in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.DataFrame({\n",
    "    'image_idx': image_idx,\n",
    "    'class': y_out\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_idx</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_idx  class\n",
       "0          0      0\n",
       "1          1      6\n",
       "2          2      0\n",
       "3          3      0\n",
       "4          4      0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataframe as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('./data/hitkul_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
